{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from utils.experimentor import *\n",
    "from utils.constants import *\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "config = {\n",
    "    \"dataset_name\": Dataset.OGBN_ARXIV,\n",
    "    \"model_type\": ModelType.GATV1,\n",
    "    'num_of_epochs': 10000,\n",
    "    'num_of_runs': 10000,\n",
    "    'patience_period': 1,\n",
    "    \n",
    "    #'should_visualize': False,\n",
    "    'batch_size': 32,\n",
    "    'test_batch_size': 32,\n",
    "    'num_workers': 0,\n",
    "    'force_cpu': False,\n",
    "    'test_frequency': 5,\n",
    "    'console_log_freq': 1,\n",
    "    'do_train_tqdm_logging': True,\n",
    "    \n",
    "    'lr': 0.01,\n",
    "    'num_of_layers': 2, \n",
    "    'num_heads': 1,\n",
    "    'hidden_size': 256,\n",
    "    'dropout': 0.25,  \n",
    "    \"use_layer_norm\": True,\n",
    "    \n",
    "    'nbor_degree': 2,\n",
    "    'adj_mode': AdjacencyMode.OneStep,\n",
    "    'sparse': True\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "experimentor = Experimentor(config)\n",
    "experimentor.run()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "81, 281, 281, 282, 282, 282, 283, 283, 283, 284, 284, 284, 285, 285,\n",
      "                           285, 285, 286, 286, 286, 286, 287, 287, 287, 288, 288, 288, 289, 289,\n",
      "                           289, 289, 290, 290, 290, 290, 291, 291, 291, 291, 292, 292, 292, 293,\n",
      "                           293, 293, 294, 294, 294, 295, 295, 295, 296, 296, 296, 297, 297, 297,\n",
      "                           298, 298, 298, 299, 299, 299, 300, 300, 300, 301, 301, 301, 302, 302,\n",
      "                           302, 303, 303, 303, 304, 304, 304, 305, 305, 305, 306, 306, 306, 307,\n",
      "                           307, 307, 308, 308, 308, 309, 309, 309, 310, 310, 310, 311, 311, 311,\n",
      "                           312, 312, 312, 313, 313, 313, 313, 314, 314, 314, 314, 315, 315, 315,\n",
      "                           315, 316, 316, 316, 316, 317, 317, 317, 318, 318, 318, 319, 319, 319,\n",
      "                           320, 320, 320, 321, 321, 321, 322, 322, 322, 323, 323, 323, 324, 324,\n",
      "                           324, 325, 325, 325, 326, 326, 326, 327, 327, 327, 328, 328, 328, 329,\n",
      "                           329, 329, 329, 330, 330, 330, 331, 331, 331, 332, 332, 332, 333, 333,\n",
      "                           333, 334, 334, 334, 335, 335, 335, 336, 336, 336, 337, 337, 337, 338,\n",
      "                           338, 338, 339, 339, 339, 340, 340, 340, 341, 341, 341, 342, 342, 342,\n",
      "                           343, 343, 343, 344, 344, 344, 344, 345, 345, 346, 346, 346, 347, 347,\n",
      "                           347, 348, 348, 348, 349, 349, 349, 350, 350, 350, 351, 351, 351, 352,\n",
      "                           352, 352, 353, 353, 353, 354, 354, 354, 355, 355, 355, 356, 356, 356,\n",
      "                           357, 357, 357, 358, 358, 358, 359, 359, 359, 360, 360, 360, 361, 361,\n",
      "                           362, 362, 363, 363, 363, 363, 364, 364, 365, 365, 366, 366, 367, 367,\n",
      "                           367, 368, 368, 369, 369, 369, 370, 370, 371, 371, 372, 372, 373, 373,\n",
      "                           374, 374, 375, 375, 375, 375, 376, 376, 376, 377, 377, 378, 378, 378,\n",
      "                           379, 379, 379, 379, 380, 380, 380, 381, 381, 381, 382, 382, 382, 383,\n",
      "                           383, 383, 384, 384, 385, 385, 386, 386, 387, 387, 388, 388, 389, 389,\n",
      "                           390, 390], device='cuda:0'),\n",
      "             col=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "                            14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "                            28,  29,  30,  31,   1,  32,   1,  33,   3,  34,   4,  35,  36,   4,\n",
      "                            37,  38,   4,  39,   4,  40,   4,  41,   4,  42,   4,  43,   4,  44,\n",
      "                             6,  27,  45, 108,   6,  46,   6,  47,   6,  48,  49,  50,  57,  51,\n",
      "                            52,  53,  54,  55,   7,  56,  57,  57,  58,   8,  59,   8,  60,   8,\n",
      "                            61,   9,  62,   9,  63,  10,  64,  12,  65,  12,  65,  66,  68,  12,\n",
      "                            65,  67,  12,  65,  68,  14,  69,  14,  70,  14,  71,  14,  72,  14,\n",
      "                            73,  16,  74,  16,  75,  17,  76,  17,  77,  17,  78,  17,  79,  17,\n",
      "                            80,  17,  81,  17,  82,  17,  83,  18,  84,  86,  18,  85,  86,  18,\n",
      "                            86,  18,  86,  87,  18,  86,  88,  19,  89,  19,  90,  19,  90,  91,\n",
      "                            19,  92,  20,  93,  20,  94,  20,  95,  21,  96,  22,  97,  99,  22,\n",
      "                            98,  22,  99,  25, 100,  25, 101,  25, 102,  25, 103,  25, 104,  26,\n",
      "                           105,  26, 106,  27, 107,  27, 108,  27, 109,  27, 110,  28, 111, 120,\n",
      "                            28, 112, 120,  28, 112, 113, 120,  28, 114,  28, 115, 120,  28, 116,\n",
      "                           120,  28, 117, 118, 119,  28, 120,  29, 121,  30, 122,  30, 123,  14,\n",
      "                            30,  71, 124,  30, 125, 130,  30, 126,  30, 127,  30, 128,  30, 125,\n",
      "                           129, 130,  30, 130, 131, 132,  31, 133, 138, 134, 135, 134, 136, 138,\n",
      "                           137, 138,  31, 139, 140,   4, 141,   4, 142,   7, 143,   7, 144,   7,\n",
      "                           145,   7, 146,   7, 147,   7, 148,   7, 149,   7, 150,   7, 151,  28,\n",
      "                           152,  28, 153,  31, 154,  31, 155,  31, 156,  31, 132, 157,  31, 137,\n",
      "                           158,  31, 159,  31, 138, 160,  31, 161,   1,  33, 162,   1,  33, 163,\n",
      "                             1,  33, 164,   1,  33, 165,   1,  33, 166,   1,  33, 167,   1,  33,\n",
      "                           168,   1,  33, 169,   4,  35, 170,   4,  35, 171,   4,  39, 172,   6,\n",
      "                            45, 108, 173,   6,  46, 174,   6,  46, 175,   6,  47, 176,   6,  47,\n",
      "                           177,   6,  47, 178,   6,  47, 179,   6,  47, 180,   6,  47, 181,   6,\n",
      "                            47, 182,   6,  47, 183,   6,  47, 184,   6,  47, 185,  50,  57, 186,\n",
      "                             7,  50,  56,  57, 187,   7,  56,  57, 188,  57, 189,  57, 190,  57,\n",
      "                           191,  57, 192,  57, 193,  57, 194,  57, 195,  57, 196,   9,  62, 197,\n",
      "                             9,  62, 198,   9,  62, 199,   9,  63, 200,   9,  63, 201,  10,  64,\n",
      "                           202,  12,  65, 203,  12,  65, 204,  14,  69, 205,  14,  69, 206,  14,\n",
      "                            69, 207,  14,  69,  70, 208,  14,  69, 209,  14,  69,  70, 210,  14,\n",
      "                            69, 211,  14,  69, 212,  14,  69,  70, 213,  14,  69, 214,  14,  70,\n",
      "                           215,  14,  70, 216,  14,  70, 217,  14,  70, 218,  14,  70, 219,  14,\n",
      "                            70, 220,  14,  70, 221,  14,  71, 222,  14,  71, 223,  14,  71, 224,\n",
      "                            14,  71, 225,  14,  71, 226,  14,  71, 227,  14,  71, 228,  14,  71,\n",
      "                           229,  14,  71, 230,  14,  72, 231,  14,  72, 232,  14,  72, 233,  14,\n",
      "                            72, 234,  14,  72, 235,  14,  72, 236,  14,  72, 237,  14,  72, 238,\n",
      "                            14,  72, 239,  14,  72, 240,  14,  73, 241,  14,  73, 242,  14,  73,\n",
      "                           243,  14,  73, 244,  14,  73, 245,  14,  73, 246,  14,  73, 247,  14,\n",
      "                            73, 248,  14,  73, 249,  14,  73, 250,  16,  75, 251,  16,  75, 252,\n",
      "                            16,  75, 253,  16,  75, 254,  17,  76, 255,  17,  76, 256,  17,  78,\n",
      "                           257,  17,  78, 258,  17,  78, 259,  17,  78, 260,  17,  78, 261,  17,\n",
      "                            78, 262,  17,  80, 263,  17,  80, 264,  17,  80, 265,  17,  80, 266,\n",
      "                            17,  80, 267,  17,  80, 268,  17,  80, 269,  17,  80, 270,  17,  80,\n",
      "                           271,  17,  80, 272,  17,  82, 273,  17,  82, 274,  17,  82, 275,  17,\n",
      "                            82, 276,  17,  82, 277,  17,  83, 278,  18,  86, 279,  18,  86, 280,\n",
      "                            18,  86, 281,  18,  86, 282,  18,  86, 283,  18,  86, 284,  18,  86,\n",
      "                            87, 285,  19,  90,  91, 286,  19,  90, 287,  19,  90, 288,  19,  90,\n",
      "                            91, 289,  19,  90,  91, 290,  19,  90,  91, 291,  20,  93, 292,  20,\n",
      "                            94, 293,  20,  94, 294,  20,  94, 295,  20,  94, 296,  20,  94, 297,\n",
      "                            20,  94, 298,  20,  94, 299,  20,  94, 300,  20,  94, 301,  20,  94,\n",
      "                           302,  20,  95, 303,  20,  95, 304,  20,  95, 305,  20,  95, 306,  20,\n",
      "                            95, 307,  20,  95, 308,  20,  95, 309,  20,  95, 310,  20,  95, 311,\n",
      "                            20,  95, 312,  22,  97,  99, 313,  22,  97,  99, 314,  22,  97,  99,\n",
      "                           315,  22,  97,  99, 316,  22,  98, 317,  22,  98, 318,  22,  98, 319,\n",
      "                            22,  98, 320,  22,  98, 321,  22,  98, 322,  22,  98, 323,  22,  98,\n",
      "                           324,  22,  98, 325,  22,  98, 326,  25, 100, 327,  25, 100, 328,  25,\n",
      "                           100, 102, 329,  25, 101, 330,  25, 101, 331,  25, 101, 332,  25, 101,\n",
      "                           333,  25, 101, 334,  25, 101, 335,  25, 101, 336,  25, 102, 337,  25,\n",
      "                           102, 338,  25, 104, 339,  25, 104, 340,  26, 105, 341,  27, 107, 342,\n",
      "                            27, 108, 343,  28, 112, 120, 344, 118, 345,  28, 120, 346,  28, 120,\n",
      "                           347,  29, 121, 348,  29, 121, 349,  29, 121, 350,  29, 121, 351,  29,\n",
      "                           121, 352,  29, 121, 353,  29, 121, 354,  30, 123, 355,  30, 123, 356,\n",
      "                            30, 123, 357,  30, 123, 358,  30, 126, 359,  30, 128, 360, 131, 361,\n",
      "                           131, 362, 132, 138, 140, 363, 132, 364, 132, 365, 134, 366, 134, 138,\n",
      "                           367, 135, 368, 135, 138, 369, 137, 370, 137, 371, 137, 372, 137, 373,\n",
      "                           138, 374,  31, 138, 139, 375, 138, 140, 376, 138, 377,  31, 139, 378,\n",
      "                            31, 139, 140, 379,  31, 139, 380,  31, 139, 381,  31, 139, 382,  31,\n",
      "                           139, 383, 140, 384, 140, 385, 140, 386, 140, 387, 140, 388, 140, 389,\n",
      "                           140, 390], device='cuda:0'),\n",
      "             val=tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.5000, 1.5000, 1.0000, 1.0000, 1.5000, 1.0000,\n",
      "                           1.0000, 1.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.5000, 1.0000,\n",
      "                           1.0000, 1.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.5000, 1.0000, 1.0000,\n",
      "                           1.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.5000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.5000, 1.0000, 1.0000, 1.5000, 1.0000,\n",
      "                           1.0000, 1.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 1.5000, 1.0000,\n",
      "                           1.0000, 1.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.5000, 1.0000, 1.0000, 1.5000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 0.5000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 0.5000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 0.5000, 1.0000, 1.0000, 0.5000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000,\n",
      "                           0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 0.5000, 1.0000, 0.5000, 1.0000,\n",
      "                           0.5000, 1.0000, 0.5000, 1.0000, 0.5000, 1.0000, 0.5000, 1.0000, 0.5000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 0.5000, 1.0000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 0.5000, 1.0000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000,\n",
      "                           1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "                           1.0000], device='cuda:0'),\n",
      "             size=(391, 391), nnz=982, density=0.64%)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Encountered tensor with size 141 in dimension 0, but expected size 391.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20892/2572127223.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mexperimentor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExperimentor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mexperimentor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\doria\\Desktop\\ProseminarMA\\multiscale-softattention\\utils\\experimentor.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mwaited_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"num_of_epochs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                 \u001b[0mdo_logging\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"console_log_freq\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdo_logging\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\doria\\Desktop\\ProseminarMA\\multiscale-softattention\\utils\\experimentor.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0madjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0madj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madjs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\doria\\Desktop\\ProseminarMA\\multiscale-softattention\\models\\base\\Net.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, adjs)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_step_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbor_degree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_layer_norm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers_normalization\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\doria\\Desktop\\ProseminarMA\\multiscale-softattention\\models\\convs\\GatConv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, edge_index, edge_weight, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         out = self.propagate(edge_index=edge_index, x=(x_l, x_r),\n\u001b[0m\u001b[0;32m    153\u001b[0m                              alpha=(alpha_l, alpha_r), size=size, edge_weight=edge_weight)\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PS\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m# Otherwise, run both functions in separation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m             coll_dict = self.__collect__(self.__user_args__, edge_index, size,\n\u001b[0m\u001b[0;32m    234\u001b[0m                                          kwargs)\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PS\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36m__collect__\u001b[1;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__set_size__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m                     data = self.__lift__(data, edge_index,\n\u001b[0;32m    159\u001b[0m                                          j if arg[-2:] == '_j' else i)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PS\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36m__set_size__\u001b[1;34m(self, size, dim, src)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mthe_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    121\u001b[0m                 (f'Encountered tensor with size {src.size(self.node_dim)} in '\n\u001b[0;32m    122\u001b[0m                  f'dimension {self.node_dim}, but expected size {the_size}.'))\n",
      "\u001b[1;31mValueError\u001b[0m: Encountered tensor with size 141 in dimension 0, but expected size 391."
     ]
    }
   ],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('PS': conda)"
  },
  "interpreter": {
   "hash": "1322c26f02ebbf0700fad666a69e487b987002dd3640f1a2df48e494c6ae2c9b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}